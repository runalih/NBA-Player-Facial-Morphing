{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QcOVsko3bIj9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# modify to where you store your project data including utils\n","datadir = \"/content/drive/My Drive/cs445/445 final/\" "],"metadata":{"id":"DU5PSaBJcSJC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","from scipy.spatial import Delaunay\n","from imutils import face_utils\n","import imutils\n","import dlib\n","\n","# function that utilizes dlib to create landmarks\n","def create_landmarks(im_path):\n","    # Initialize face detector and shape predictor\n","    detector_value = dlib.get_frontal_face_detector()\n","    predictor_value = dlib.shape_predictor(\"/Users/rohansathe/Downloads/shape_predictor_68_face_landmarks.dat\")\n","\n","    # Read in the image\n","    im = cv2.imread(im_path)\n","\n","    # Detect faces in the image\n","    rectangles = detector_value(im, 1)\n","\n","    # Get face landmarks\n","    im_landmark_arr = []\n","    for (i, rect) in enumerate(rectangles):\n","        shape = predictor_value(im, rect)\n","        # convert shape predictor into something readable\n","        shape = face_utils.shape_to_np(shape)\n","        for (x, y) in shape:\n","            im_landmark_arr.append((x,y))\n","    \n","    im_landmark_arr = np.array(im_landmark_arr)\n","    return im_landmark_arr\n","    \n","# this function will actually warp the two images together using the original image landmarks and the weighted image landmarks\n","def warp_image(im, landmark_list1, weighted_landmark_list, dst_triangulation):\n","    im_out = im.copy()\n","\n","    # Warp each triangle from first im landmarks to second im landmarks \n","    for i in range(len(dst_triangulation)):\n","        # finds points that correspond to indices of delaunay triangles\n","        landmark_list1_app = landmark_list1[dst_triangulation[i]]\n","        weighted_landmark_list_app = weighted_landmark_list[dst_triangulation[i]]\n","\n","        # Put bonding rectangles around the first and second triangles\n","        rect1 = cv2.boundingRect(np.float32([landmark_list1_app]))\n","        rect_weighted = cv2.boundingRect(np.float32([weighted_landmark_list_app]))\n","\n","        # Initialize new lists\n","        bounded_lmlist1 = []\n","        bounded_lmlistweighted = []\n","\n","        # Get new triangle coordinates in the bounding box\n","        for i in range(3):\n","            bounded_lmlist1.append((landmark_list1_app[i][0] - rect1[0], landmark_list1_app[i][1] - rect1[1]))\n","            bounded_lmlistweighted.append((weighted_landmark_list_app[i][0] - rect_weighted[0], weighted_landmark_list_app[i][1] - rect_weighted[1]))\n","\n","         # Create and Fill mask for second image (i.e. bounded_lmlistweighted)\n","        mask = np.zeros((rect_weighted[3], rect_weighted[2], 3), dtype=np.float32)\n","        cv2.fillConvexPoly(mask, np.int32(bounded_lmlistweighted), (1.0, 1.0, 1.0), 16, 0)\n","\n","        # apply bounding box constraints to first image\n","        im_cropped = im[rect1[1]:rect1[1] + rect1[3], rect1[0]:rect1[0] + rect1[2]]\n","\n","        # affine transformation using bounded landmarks\n","        im_size = (rect_weighted[2], rect_weighted[3])\n","        M = cv2.getAffineTransform(np.float32(bounded_lmlist1), np.float32(bounded_lmlistweighted))\n","        # warp affine transform\n","        warped_im = cv2.warpAffine(im_cropped, M, im_size, borderMode=cv2.BORDER_REFLECT_101)\n","\n","        # Blend the warped image patch with the output image\n","        im_out[rect_weighted[1]:rect_weighted[1]+rect_weighted[3], rect_weighted[0]:rect_weighted[0]+rect_weighted[2]] = im_out[rect_weighted[1]:rect_weighted[1]+rect_weighted[3], rect_weighted[0]:rect_weighted[0]+rect_weighted[2]] * (1 - mask) + warped_im* mask\n","\n","    return im_out\n","\n","# primary driver function\n","def fin_func():\n","    # Define file paths for input images and put them into enumerated list\n","    filename_list = [\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/beal.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/brunson.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/devinbooker.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/embiid.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/jimmybuckets.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/jokic.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/kevindurant.jpg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/klay.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/lebron.jpeg',\n","        '/Users/rohansathe/Documents/CS445/MP5/nbajpeg/tatum.jpeg'\n","    ]\n","\n","    num = 1\n","    # Iterate through each pair of consecutive images\n","    for i in range(len(filename_list)-1):\n","        print(i)\n","        # Read the images\n","        im1 = cv2.imread(filename_list[i])\n","        im2 = cv2.imread(filename_list[i+1])\n","\n","        # Get face landmarks\n","        landmark_list1 = create_landmarks(filename_list[i])\n","        landmark_list2 = create_landmarks(filename_list[i+1])\n","        landmark_list1 = np.array(landmark_list1)\n","        landmark_list2 = np.array(landmark_list2)\n","\n","        # Compute average landmarks\n","        average_landmarks = (landmark_list1 + landmark_list2) / 2 \n","\n","        # Delaunay triangulation with average landmarks between original and second images \n","        triangulation = Delaunay(average_landmarks).simplices\n","        alpha = 0.0\n","        while alpha <= 1.0:\n","            # applies a certain level of alpha to original landmark list v. final --> the smaller alpha is the less of that image is included in the morph \n","            weighted_landmark_list = (1.0 - alpha) * landmark_list1 + alpha * landmark_list2\n","            warped_image1 = warp_image(im1, landmark_list1, weighted_landmark_list, triangulation)\n","            warped_image2 = warp_image(im2, landmark_list2, weighted_landmark_list, triangulation)\n","\n","            # similar process to one of our earlier MPs\n","            blended_image = (1.0 - alpha) * warped_image1 + alpha * warped_image2\n","            cv2.imwrite(\"/Users/rohansathe/Documents/CS445/FinalProject/nbaimagefolder/imfin\" + f'{num:05}' + \".jpg\", blended_image)\n","            alpha = alpha + 0.01\n","            num = num + 1\n","    return 0\n","\n","if __name__ == '__main__':\n","    blended_fin = fin_func()"],"metadata":{"id":"Ia318cz_cYpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["COMMAND USED TO GENERATE VIDEO FROM MORPHED IMAGES --> YOU CAN TOGGLE WITH THE ALPHA INCREASE TO GENERATE MORE OR LESS WARPING\n","\n","#ffmpeg -framerate 20 -i '/Users/rohansathe/Documents/CS445/FinalProject/nbaimagefolder/imfin%05d.jpg' -c:v libx264 -vf fps=20 -pix_fmt yuv420p '/Users/rohansathe/Documents/CS445/FinalProject/finalnbavideo.mp4'"],"metadata":{"id":"tvhzBWWZnrdk"},"execution_count":null,"outputs":[]}]}